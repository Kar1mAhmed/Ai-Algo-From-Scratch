{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in this project i will build basic decision tree with numpy only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the data set i will work on\n",
    "X_train = np.array([[1,1,1],[1,0,1],[1,0,0],[1,0,0],[1,1,1],[0,1,1],[0,0,0],[1,0,1],[0,1,0],[1,0,0]])\n",
    "y_train = np.array([1,1,0,0,1,0,0,1,1,0]) # Edible response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>The Data set image from Andrew course</h1>\n",
    "<img src=\"data_set.png\" alt=\"Alternative text\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we will need a function to compute the entropy depending on the response vector\n",
    "def compute_entropy(y):\n",
    "      entropy = 0.0\n",
    "      if len(y) != 0 :\n",
    "            p1 = len(y[y==1]) / len(y) # divide the number of all 1 responses by the data set size\n",
    "\n",
    "            if p1 == 0 or p1 == 1 :\n",
    "                  entropy = 0 # if all the responses are 1 or all are 0 then the entropy is zero\n",
    "            else:\n",
    "                  entropy = -p1 * np.log2(p1) - (1 - p1) * np.log2(1 - p1)\n",
    "\n",
    "      return entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      " We passed our simple test :)\n"
     ]
    }
   ],
   "source": [
    "# testing  compute entropy function on or data set , since half of responses are 1 and the other half is 0\n",
    "# the entropy should equal one\n",
    "print(compute_entropy(y_train))\n",
    "print(\" We passed our simple test :)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we should work on a function that split the data depending on a feature \n",
    "def split_on_feature(X ,indices , feature_index):\n",
    "      \n",
    "      left_indices = []   #Indices with feature value == 0\n",
    "      right_indices = []  #Indices with feature value == 1\n",
    "\n",
    "      for i in indices:\n",
    "            if X[i][feature_index] == 1 :\n",
    "                  right_indices.append(i)\n",
    "            else :\n",
    "                  left_indices.append(i)\n",
    "      \n",
    "      return left_indices , right_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([5, 6, 8], [0, 1, 2, 3, 4, 7, 9])\n",
      "test passed again 0_0\n"
     ]
    }
   ],
   "source": [
    "# Now let's test the split function on the brown cap feature \n",
    "# we should get index 5 , 6 , 8 on the left , others on the right\n",
    "root_indices = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "print(split_on_feature(X_train, root_indices , 0))\n",
    "print(\"test passed again 0_0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now should start implementing the function that will compute information gain on split , and it will work as follow\n",
    "# 1- we should compute entropy for the node before the split\n",
    "# 2- then we start split or data on specific feature\n",
    "# 3- compute the entropy for the right which splitted to the right and the data splitted to the left \n",
    "# 4- finally we compute information gain as follow          Information Gain= ùêª(ùëùnode1) ‚àí (ùë§_left * ùêª(ùëù_left1) + ùë§_right * ùêª(ùëù_right1))\n",
    "\n",
    "def information_gain(X, y,node_indices , feature):\n",
    "\n",
    "      right_indices , left_indices = split_on_feature(X ,node_indices, feature) #2\n",
    "\n",
    "      X_node, y_node = X[node_indices], y[node_indices]\n",
    "      X_left, y_left = X[left_indices], y[left_indices] \n",
    "      X_right, y_right = X[right_indices], y[right_indices]\n",
    "\n",
    "      node_entropy = compute_entropy(y_node) #1\n",
    "      left_entropy = compute_entropy(y_left)\n",
    "      right_entropy = compute_entropy(y_right) #3\n",
    "\n",
    "      w_left = len(X_left) / len(X_node)\n",
    "      w_right = len(X_right) / len(X_node) #getting the weight for both child\n",
    "\n",
    "      weighted_entropy = w_left * left_entropy + w_right * right_entropy\n",
    "\n",
    "      information_gain = node_entropy - weighted_entropy #4\n",
    "\n",
    "      return information_gain\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected Output:\n",
    "\n",
    "Information Gain from splitting the root on brown cap:  0.034851554559677034 <br>\n",
    "Information Gain from splitting the root on tapering stalk shape:  0.12451124978365313 <br>\n",
    "Information Gain from splitting the root on solitary:  0.2780719051126377"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.034851554559677034\n",
      "0.12451124978365313\n",
      "0.2780719051126377\n"
     ]
    }
   ],
   "source": [
    "print(information_gain(X_train , y_train , root_indices , 0))\n",
    "print(information_gain(X_train , y_train , root_indices , 1))\n",
    "print(information_gain(X_train , y_train , root_indices , 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now let's make function the return best feature for the split\n",
    "def best_split(X, y ,node_indices):\n",
    "\n",
    "      features_num = X.shape[1]\n",
    "\n",
    "      best_feature = -1\n",
    "      highest_gain= -1\n",
    "      for i in range(features_num):\n",
    "            current_gain = information_gain(X , y, node_indices , i)\n",
    "\n",
    "            if current_gain > highest_gain :\n",
    "                  highest_gain = current_gain \n",
    "                  best_feature = i\n",
    "      \n",
    "      return best_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "#best feature should be solitary  ( index : 2)\n",
    "print(best_split(X_train,y_train,root_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fcf4e9ccc72525710c9361c09f6503844aacd437ecce5c29a00e19bd3cec452e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
